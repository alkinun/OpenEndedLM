{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.45.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.668 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers, TRL and unsloth via:\n",
      "`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer=FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model=FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0, # Currently only supports dropout=0\n",
    "    bias=\"none\",    # Currently only supports bias=\"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ee4ba6d617469ea83a299e78d3faea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{}<|im_end|>\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def format_prompt(examples):\n",
    "    prompts   = examples[\"prompt\"]\n",
    "    responses = examples[\"chosen\"]\n",
    "    texts = []\n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = prompt_template.format(prompt, response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"OpenEndedLM/OpenEndPreference-v0.1\")[\"train\"]\n",
    "dataset = dataset.map(format_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the significance of the 13 stripes on the American flag?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The 13 stripes on the American flag represent the original 13 colonies that declared their independence from Great Britain and became the first states in the United States. These colonies were:\n",
      "\n",
      "1. Delaware\n",
      "2. Pennsylvania\n",
      "3. New Jersey\n",
      "4. Georgia\n",
      "5. Connecticut\n",
      "6. Massachusetts\n",
      "7. Maryland\n",
      "8. South Carolina\n",
      "9. New Hampshire\n",
      "10. Virginia\n",
      "11. New York\n",
      "12. North Carolina\n",
      "13. Rhode Island\n",
      "\n",
      "The red and white stripes symbolize the struggle and unity of these colonies against British rule, with each stripe representing one of the original states. The alternating pattern of red and white is a traditional design for flags, but in this context, it also serves as a visual reminder of the historical union and the principles upon which the United States was founded.\n",
      "\n",
      "The flag's design has evolved over time to reflect changes within the country, such as the addition of stars (one for each state) and new stripes when new states join, though since 1795 no additional stripes have been added due to the decision to maintain the original 13.<|im_end|><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d783692e5bb74ee3bca8e255848a0d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 36\n",
      " \"-____-\"     Number of trainable parameters = 73,859,072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
      "`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5070e97365f438c838d39598711e289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5018, 'grad_norm': 0.36850401759147644, 'learning_rate': 4e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5265, 'grad_norm': 0.3502880930900574, 'learning_rate': 8e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4692, 'grad_norm': 0.37345170974731445, 'learning_rate': 0.00012, 'epoch': 0.24}\n",
      "{'loss': 0.5662, 'grad_norm': 0.43245166540145874, 'learning_rate': 0.00016, 'epoch': 0.32}\n",
      "{'loss': 0.442, 'grad_norm': 0.3521885573863983, 'learning_rate': 0.0002, 'epoch': 0.4}\n",
      "{'loss': 0.4482, 'grad_norm': 0.43603280186653137, 'learning_rate': 0.00019354838709677422, 'epoch': 0.48}\n",
      "{'loss': 0.5398, 'grad_norm': 0.5475645065307617, 'learning_rate': 0.0001870967741935484, 'epoch': 0.56}\n",
      "{'loss': 0.5601, 'grad_norm': 0.5059583783149719, 'learning_rate': 0.00018064516129032257, 'epoch': 0.64}\n",
      "{'loss': 0.5702, 'grad_norm': 0.5280370116233826, 'learning_rate': 0.00017419354838709678, 'epoch': 0.72}\n",
      "{'loss': 0.4482, 'grad_norm': 0.402022123336792, 'learning_rate': 0.00016774193548387098, 'epoch': 0.8}\n",
      "{'loss': 0.5843, 'grad_norm': 0.4537448287010193, 'learning_rate': 0.00016129032258064516, 'epoch': 0.88}\n",
      "{'loss': 0.5032, 'grad_norm': 0.5230410099029541, 'learning_rate': 0.00015483870967741937, 'epoch': 0.96}\n",
      "{'loss': 0.4114, 'grad_norm': 0.47881457209587097, 'learning_rate': 0.00014838709677419355, 'epoch': 1.04}\n",
      "{'loss': 0.3529, 'grad_norm': 0.5073493719100952, 'learning_rate': 0.00014193548387096775, 'epoch': 1.12}\n",
      "{'loss': 0.3032, 'grad_norm': 0.4428594410419464, 'learning_rate': 0.00013548387096774193, 'epoch': 1.2}\n",
      "{'loss': 0.3025, 'grad_norm': 0.44664451479911804, 'learning_rate': 0.00012903225806451613, 'epoch': 1.28}\n",
      "{'loss': 0.2892, 'grad_norm': 0.6034093499183655, 'learning_rate': 0.00012258064516129034, 'epoch': 1.36}\n",
      "{'loss': 0.3347, 'grad_norm': 0.5523923635482788, 'learning_rate': 0.00011612903225806453, 'epoch': 1.44}\n",
      "{'loss': 0.2852, 'grad_norm': 0.7492635250091553, 'learning_rate': 0.00010967741935483871, 'epoch': 1.52}\n",
      "{'loss': 0.2104, 'grad_norm': 0.5466580390930176, 'learning_rate': 0.0001032258064516129, 'epoch': 1.6}\n",
      "{'loss': 0.2641, 'grad_norm': 0.6936362981796265, 'learning_rate': 9.677419354838711e-05, 'epoch': 1.68}\n",
      "{'loss': 0.2736, 'grad_norm': 0.8915692567825317, 'learning_rate': 9.032258064516129e-05, 'epoch': 1.76}\n",
      "{'loss': 0.3085, 'grad_norm': 0.45039498805999756, 'learning_rate': 8.387096774193549e-05, 'epoch': 1.84}\n",
      "{'loss': 0.2555, 'grad_norm': 0.47303473949432373, 'learning_rate': 7.741935483870968e-05, 'epoch': 1.92}\n",
      "{'loss': 0.2174, 'grad_norm': 0.5106782913208008, 'learning_rate': 7.096774193548388e-05, 'epoch': 2.0}\n",
      "{'loss': 0.188, 'grad_norm': 0.5323753356933594, 'learning_rate': 6.451612903225807e-05, 'epoch': 2.08}\n",
      "{'loss': 0.1592, 'grad_norm': 0.4178834557533264, 'learning_rate': 5.8064516129032266e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1687, 'grad_norm': 0.5081035494804382, 'learning_rate': 5.161290322580645e-05, 'epoch': 2.24}\n",
      "{'loss': 0.1186, 'grad_norm': 0.4140077233314514, 'learning_rate': 4.516129032258064e-05, 'epoch': 2.32}\n",
      "{'loss': 0.2028, 'grad_norm': 0.500584602355957, 'learning_rate': 3.870967741935484e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2042, 'grad_norm': 0.4745417535305023, 'learning_rate': 3.2258064516129034e-05, 'epoch': 2.48}\n",
      "{'loss': 0.1687, 'grad_norm': 0.4289059340953827, 'learning_rate': 2.5806451612903226e-05, 'epoch': 2.56}\n",
      "{'loss': 0.1263, 'grad_norm': 0.38310039043426514, 'learning_rate': 1.935483870967742e-05, 'epoch': 2.64}\n",
      "{'loss': 0.2003, 'grad_norm': 0.4825829267501831, 'learning_rate': 1.2903225806451613e-05, 'epoch': 2.72}\n",
      "{'loss': 0.1588, 'grad_norm': 0.4217219054698944, 'learning_rate': 6.451612903225806e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1429, 'grad_norm': 0.4181652367115021, 'learning_rate': 0.0, 'epoch': 2.88}\n",
      "{'train_runtime': 52.2205, 'train_samples_per_second': 5.745, 'train_steps_per_second': 0.689, 'train_loss': 0.3279662171585692, 'epoch': 2.88}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Whats a triangle<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_end|>A triangle is a polygon with three sides and three vertices. It's one of the basic shapes in geometry and has several important properties:\n",
      "\n",
      "1. **Classification**: Triangles can be classified based on their angles (acute, right, obtuse) or by their side lengths (equilateral, isosceles, scalene).\n",
      "\n",
      "2. **Properties**:\n",
      "   - All interior angles sum up to 180 degrees.\n",
      "   - Each angle is less than 180 degrees.\n",
      "   - The sum of any two sides must be greater than the third side (triangle inequality theorem).\n",
      "   \n",
      "3. **Types**:\n",
      "   - Equilateral Triangle: All three sides are equal, and all angles are 60 degrees.\n",
      "   - Isosceles Triangle: Two sides are equal, and the angles opposite these sides are also equal.\n",
      "   - Scalene Triangle: No sides are equal, and therefore no angles are equal.\n",
      "\n",
      "4. **Geometric Shapes**: A triangle is fundamental in various fields such as mathematics, physics, engineering, and architecture because it forms the basic unit of polygons and is used extensively in formulas and calculations.\n",
      "\n",
      "Triangles are not only simple but also incredibly versatile; they appear frequently in nature, art, design, and technology due to their stability and simplicity.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt=Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs=tokenizer(\n",
    "[\n",
    "    prompt_template.format(\n",
    "        \"Whats a triangle\", # prompt\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs=model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You're not saving a tokenizer as well?\n",
      "You can do it separately via `tokenizer.save_pretrained(...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 11.8 out of 31.11 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 183.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
